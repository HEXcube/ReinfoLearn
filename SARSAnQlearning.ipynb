{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA & Q-learning\n",
    "\n",
    "This assignment implements SARSA and Q-learning algorithms from Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2D Array operations - https://www.tutorialspoint.com/python/python_2darray.htm\n",
    "\n",
    "def showMatrix(matrix):\n",
    "    for row in matrix:\n",
    "        for col in row:\n",
    "            print(col,end = '\\t')\n",
    "        print()\n",
    "\n",
    "\n",
    "def createMatrix(matrix_size):\n",
    "    # Create an NxN matrix with all values initialized to 0\n",
    "    # https://www.geeksforgeeks.org/python-using-2d-arrays-lists-the-right-way/\n",
    "    squareMatrix = [[0 for i in range(matrix_size)] for j in range(matrix_size)]\n",
    "    return squareMatrix\n",
    "\n",
    "def prettifyMatrix(matrix):\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            matrix[i][j] = round(matrix[i][j], 4)\n",
    "    return matrix\n",
    "\n",
    "def prettify3dMatrix(matrix):\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            for k in range(len(matrix[i][j])):\n",
    "                matrix[i][j][k] = round(matrix[i][j][k], 2)\n",
    "    return matrix\n",
    "\n",
    "def arbitrarilyInitializeMatrix(matrix, initial_q_range):\n",
    "    # Import function to generate random integers\n",
    "    # https://www.geeksforgeeks.org/python-randint-function/\n",
    "    from random import randint\n",
    "\n",
    "    # Arbitrarily initialize Q values in the format ['→', '↓', '←', '↑']\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix)):\n",
    "            # https://stackoverflow.com/questions/16655089/python-random-numbers-into-a-list/16655135#16655135\n",
    "            matrix[i][j] = [randint(0, initial_q_range) for count in range(4)]\n",
    "    return matrix\n",
    "\n",
    "def nextStateCoordinates(current_row, current_col, direction, grid_size):\n",
    "    i = current_row\n",
    "    j = current_col\n",
    "    #check for out of bounds and update coordinates\n",
    "    if((direction == '←') and (current_col > 0)):\n",
    "        j-=1\n",
    "    elif((direction == '↑') and (current_row > 0)):\n",
    "        i-=1\n",
    "    elif((direction == '→') and (current_col < grid_size-1)):\n",
    "        j+=1\n",
    "    elif(current_row < grid_size-1):\n",
    "        i+=1\n",
    "    return (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform as probability\n",
    "from random import choice as randomChoice\n",
    "\n",
    "def findNextAction(qListOfCurrentState, epsilon):\n",
    "    actionList = ['→', '↓', '←', '↑']\n",
    "    # https://stackoverflow.com/questions/33359740/random-number-between-0-and-1-in-python/33359801#33359801\n",
    "    if(probability(0, 1) < epsilon):\n",
    "        # Explore for ε times\n",
    "        # https://www.geeksforgeeks.org/random-numbers-in-python/\n",
    "        return randomChoice(actionList)\n",
    "    else:\n",
    "        # Exploit for 1-ε times (greedy action)\n",
    "        # https://stackoverflow.com/questions/2474015/getting-the-index-of-the-returned-max-or-min-item-using-max-min-on-a-list/2474030#2474030\n",
    "        max_index = qListOfCurrentState.index(max(qListOfCurrentState))\n",
    "        return actionList[max_index]\n",
    "\n",
    "def sarsa(grid_size, startState, goalState, reward, discount_factor, step_size, epsilon, initial_q_range, episode_range):\n",
    "\n",
    "    qGrid = createMatrix(grid_size)\n",
    "    qGrid[startState[0]][startState[1]] = 's'\n",
    "    qGrid[goalState[0]][goalState[1]] = 'x'\n",
    "\n",
    "    print(\"Matrix representation of given grid world :\")\n",
    "    showMatrix(qGrid)\n",
    "    \n",
    "    qGrid = arbitrarilyInitializeMatrix(qGrid, initial_q_range)\n",
    "\n",
    "    # Set Q values of goal state to 0\n",
    "    qGrid[goalState[0]][goalState[1]] = [0,0,0,0]\n",
    "\n",
    "    print(\"\\nArbitrarily initialized state-action Q values in the format ['→', '↓', '←', '↑'] :\")\n",
    "    showMatrix(qGrid)\n",
    "\n",
    "    print(\"\\nIntermediate state-action Q values :\")\n",
    "    for episode_count in range(episode_range):\n",
    "        currentState = startState\n",
    "        current_action = findNextAction(qGrid[currentState[0]][currentState[1]], epsilon)\n",
    "\n",
    "        # Simulate do-while on Python\n",
    "        # https://www.javatpoint.com/python-do-while-loop\n",
    "        while(True):\n",
    "            nextState = nextStateCoordinates(currentState[0], currentState[1], current_action, grid_size)\n",
    "            next_action = findNextAction(qGrid[nextState[0]][nextState[1]], epsilon)\n",
    "            current_action_index = ['→', '↓', '←', '↑'].index(current_action)\n",
    "            next_action_index = ['→', '↓', '←', '↑'].index(next_action)            \n",
    "            currentQ = qGrid[currentState[0]][currentState[1]][current_action_index]\n",
    "            nextQ = qGrid[nextState[0]][nextState[1]][next_action_index]\n",
    "            qGrid[currentState[0]][currentState[1]][current_action_index] = currentQ + step_size * (reward + discount_factor * nextQ - currentQ)\n",
    "            currentState = nextState\n",
    "            current_action = next_action\n",
    "            if(currentState == goalState):\n",
    "                break\n",
    "\n",
    "        print(\"\\nEpisode\", episode_count, \":\")\n",
    "        showMatrix(prettify3dMatrix(qGrid))\n",
    "\n",
    "    return prettify3dMatrix(qGrid)\n",
    "\n",
    "def qLearning(grid_size, startState, goalState, reward, discount_factor, step_size, epsilon, initial_q_range, episode_range):\n",
    "\n",
    "    qGrid = createMatrix(grid_size)\n",
    "    qGrid[startState[0]][startState[1]] = 's'\n",
    "    qGrid[goalState[0]][goalState[1]] = 'x'\n",
    "\n",
    "    print(\"Matrix representation of given grid world :\")\n",
    "    showMatrix(qGrid)\n",
    "    \n",
    "    qGrid = arbitrarilyInitializeMatrix(qGrid, initial_q_range)\n",
    "\n",
    "    # Set Q values of goal state to 0\n",
    "    qGrid[goalState[0]][goalState[1]] = [0,0,0,0]\n",
    "\n",
    "    print(\"\\nArbitrarily initialized state-action Q values in the format ['→', '↓', '←', '↑'] :\")\n",
    "    showMatrix(qGrid)\n",
    "\n",
    "    print(\"\\nIntermediate state-action Q values :\")\n",
    "    for episode_count in range(episode_range):\n",
    "        currentState = startState\n",
    "\n",
    "        # Simulate do-while on Python\n",
    "        # https://www.javatpoint.com/python-do-while-loop\n",
    "        while(True):\n",
    "            current_action = findNextAction(qGrid[currentState[0]][currentState[1]], epsilon)\n",
    "            current_action_index = ['→', '↓', '←', '↑'].index(current_action)\n",
    "            nextState = nextStateCoordinates(currentState[0], currentState[1], current_action, grid_size)   \n",
    "            currentQ = qGrid[currentState[0]][currentState[1]][current_action_index]\n",
    "            maxNextQ = max(qGrid[nextState[0]][nextState[1]])\n",
    "            qGrid[currentState[0]][currentState[1]][current_action_index] = currentQ + step_size * (reward + discount_factor * maxNextQ - currentQ)\n",
    "            currentState = nextState\n",
    "            if(currentState == goalState):\n",
    "                break\n",
    "\n",
    "        print(\"\\nEpisode\", episode_count, \":\")\n",
    "        showMatrix(prettify3dMatrix(qGrid))\n",
    "\n",
    "    return prettify3dMatrix(qGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA\n",
      "=====\n",
      "\n",
      "Matrix representation of given grid world :\n",
      "0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t\n",
      "0\t0\t0\tx\t0\t\n",
      "0\ts\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t\n",
      "\n",
      "Arbitrarily initialized state-action Q values in the format ['→', '↓', '←', '↑'] :\n",
      "[3, 1, 3, 1]\t[0, 2, 1, 4]\t[2, 0, 4, 4]\t[4, 0, 2, 1]\t[3, 0, 4, 3]\t\n",
      "[3, 0, 0, 4]\t[4, 1, 3, 2]\t[1, 0, 2, 2]\t[1, 3, 1, 2]\t[1, 1, 2, 0]\t\n",
      "[2, 1, 1, 4]\t[4, 0, 2, 1]\t[3, 3, 3, 4]\t[0, 0, 0, 0]\t[4, 4, 0, 0]\t\n",
      "[0, 0, 3, 2]\t[0, 0, 1, 1]\t[0, 1, 3, 1]\t[2, 3, 2, 3]\t[3, 3, 0, 2]\t\n",
      "[3, 4, 1, 0]\t[0, 1, 3, 3]\t[0, 3, 1, 0]\t[4, 0, 1, 2]\t[2, 3, 1, 4]\t\n",
      "\n",
      "Intermediate state-action Q values :\n",
      "\n",
      "Episode 0 :\n",
      "[3, 1, 3, 1]\t[0, 2, 1, 0.72]\t[2, 0, 2.12, 4]\t[4, 0, 2, 1]\t[3, 0, 4, 3]\t\n",
      "[3, 0, 0, 4]\t[1.19, 0.5, 3, 2]\t[1, 0, 1.62, 1.62]\t[1, 3, 1, 2]\t[1, 1, 2, 0]\t\n",
      "[2, 1, 1, 4]\t[1.19, 0, 2, 1]\t[0.0, 3, 3, 1.19]\t[0, 0, 0, 0]\t[4, 4, 0, 0]\t\n",
      "[0, 0, 1.41, 2]\t[0, 0, 0.91, 1.38]\t[0, 1, 3, 1]\t[2, 3, 2, 3]\t[3, 3, 0, 2]\t\n",
      "[1.01, 0.1, 1, 0]\t[0, 1, 1.88, 0.47]\t[0, 3, 1, 0]\t[4, 0, 1, 2]\t[2, 3, 1, 4]\t\n",
      "\n",
      "Episode 1 :\n",
      "[-0.3, 0.91, 0.44, 1]\t[0, -0.57, -0.3, -0.01]\t[2, 0, 0.07, 1.01]\t[4, 0, 2, 1]\t[3, 0, 4, 3]\t\n",
      "[-0.77, 0.19, -0.28, 0.84]\t[-0.74, -0.2, -0.74, -0.15]\t[0.91, 0.66, -0.11, 0.63]\t[1, 0.0, 1, 2]\t[1, 1, 2, 0]\t\n",
      "[0.22, -1.0, 0.16, -0.78]\t[0.05, 0, 0.12, -0.38]\t[0.0, 1.41, 0.45, 0.02]\t[0, 0, 0, 0]\t[4, 4, 0, 0]\t\n",
      "[-0.82, -1.17, -0.6, -1.02]\t[0, 0, -0.71, -0.16]\t[0, 1, 0.43, 1]\t[2, 3, 2, 3]\t[3, 3, 0, 2]\t\n",
      "[-0.39, -0.92, -0.58, -1.2]\t[0, -0.67, -0.66, -0.38]\t[0, 3, 1, 0]\t[4, 0, 1, 2]\t[2, 3, 1, 4]\t\n",
      "\n",
      "Episode 2 :\n",
      "[-0.3, 0.91, 0.44, 1]\t[0, -0.57, -0.3, -0.01]\t[2, 0, 0.07, 1.01]\t[4, 0, 2, 1]\t[3, 0, 4, 3]\t\n",
      "[-0.77, 0.19, -0.28, 0.84]\t[-0.74, -0.2, -0.74, -0.15]\t[0.91, 0.66, -0.11, 0.63]\t[1, 0.0, 1, 2]\t[1, 1, 2, 0]\t\n",
      "[0.22, -1.0, 0.16, -0.78]\t[0.05, 0, 0.12, -0.38]\t[0.0, 1.41, 0.45, 0.02]\t[0, 0, 0, 0]\t[4, 4, -0.75, 0]\t\n",
      "[-0.82, -1.17, -0.6, -1.02]\t[-0.28, 0, -0.71, -0.16]\t[0, 0.91, 0.43, 1]\t[2, 3, 2, 3]\t[1.41, 0.25, 0, -0.25]\t\n",
      "[-0.39, -0.92, -0.58, -1.2]\t[0, -0.67, -0.66, -0.38]\t[1.12, 0.0, 1, 0]\t[2.12, 0, 1, 2]\t[0.2, -0.11, 1, 0.46]\t\n",
      "\n",
      "Episode 3 :\n",
      "[-0.3, 0.91, 0.44, 1]\t[0, -0.57, -0.3, -0.01]\t[2, 0, 0.07, 1.01]\t[4, 0, 2, 1]\t[3, 0, 4, 3]\t\n",
      "[-0.77, 0.19, -0.28, 0.84]\t[-0.74, -0.2, -0.74, -0.15]\t[0.91, 0.66, -0.11, 0.63]\t[1, 0.0, 1, 2]\t[1, 1, 2, 0]\t\n",
      "[0.22, -1.55, 0.16, -0.78]\t[0.05, 0, -1.19, -0.38]\t[0.0, 1.41, 0.45, 0.02]\t[0, 0, 0, 0]\t[4, 4, -0.75, 0]\t\n",
      "[-0.82, -1.23, -0.6, -1.02]\t[-0.28, -0.75, -0.71, -0.73]\t[0, 0.91, 0.43, 1]\t[2, 0.54, 2, 0.0]\t[1.41, -0.22, 0, -0.25]\t\n",
      "[-1.03, -0.92, -0.58, -1.2]\t[-0.22, -0.67, -0.66, -0.84]\t[0.32, 0.0, 1, 0]\t[-0.66, 0, -0.25, 0.95]\t[-0.95, -1.19, -0.41, -0.52]\t\n",
      "\n",
      "Episode 4 :\n",
      "[-0.3, 0.91, 0.44, 1]\t[0, -0.57, -0.3, -0.01]\t[2, 0, 0.07, 1.01]\t[4, 0, 2, 1]\t[3, 0, 4, 3]\t\n",
      "[-0.77, 0.19, -0.28, 0.84]\t[-0.74, -0.2, -0.74, -0.15]\t[0.91, 0.66, -0.11, 0.63]\t[1, 0.0, 1, 2]\t[1, 1, 2, 0]\t\n",
      "[0.22, -1.55, 0.16, -0.78]\t[-0.74, 0, -1.19, -0.38]\t[-0.75, -0.32, -0.61, 0.02]\t[0, 0, 0, 0]\t[4, 4, -0.75, 0]\t\n",
      "[-0.82, -1.23, -0.6, -1.02]\t[-0.35, -0.75, -0.71, -0.73]\t[0, 0.91, 0.43, -0.5]\t[2, 0.54, 2, 0.0]\t[1.41, -0.22, 0, -0.25]\t\n",
      "[-1.03, -0.92, -0.58, -1.2]\t[-0.22, -0.67, -0.66, -0.84]\t[0.32, 0.0, 1, 0]\t[-0.66, 0, -0.25, 0.95]\t[-0.95, -1.19, -0.41, -0.52]\t\n",
      "\n",
      "Episode 5 :\n",
      "[-0.3, 0.91, 0.44, 1]\t[0, -0.57, -0.3, -0.01]\t[2, 0, 0.07, 1.01]\t[4, 0, 2, 1]\t[3, 0, 4, 3]\t\n",
      "[-0.77, 0.19, -0.28, 0.84]\t[-0.74, -0.2, -0.74, -0.15]\t[0.91, 0.66, -0.11, 0.63]\t[1, 0.0, 1, 2]\t[1, 1, 2, 0]\t\n",
      "[0.22, -1.55, 0.16, -0.78]\t[-0.74, 0, -1.19, -0.38]\t[-0.75, -0.32, -0.61, 0.02]\t[0, 0, 0, 0]\t[4, 4, -0.75, 0]\t\n",
      "[-0.82, -1.23, -0.6, -1.02]\t[-0.85, -0.75, -0.71, -0.73]\t[-0.45, -0.52, -0.84, -0.5]\t[-0.25, -0.17, -0.16, -0.75]\t[1.41, -0.22, 0.19, -0.25]\t\n",
      "[-1.03, -0.92, -0.58, -1.2]\t[-0.22, -0.67, -0.66, -0.84]\t[0.32, 0.0, 1, -0.55]\t[-0.66, 0, -0.25, -0.51]\t[-0.95, -1.19, -0.41, -0.52]\t\n",
      "\n",
      "Episode 6 :\n",
      "[-0.3, 0.91, 0.44, 1]\t[0, -0.57, -0.3, -0.01]\t[2, 0, 0.07, 1.01]\t[4, 0, 2, 1]\t[3, 0, 4, 3]\t\n",
      "[-0.77, 0.19, -0.28, 0.84]\t[-0.74, -0.2, -0.74, -0.15]\t[0.91, 0.66, -0.11, 0.63]\t[1, 0.0, 1, 2]\t[1, 1, 2, 0]\t\n",
      "[0.22, -1.55, 0.16, -0.78]\t[-0.74, -1.1, -1.19, -0.38]\t[-0.75, -0.32, -0.61, 0.02]\t[0, 0, 0, 0]\t[4, 4, -0.75, 0]\t\n",
      "[-1.3, -1.23, -1.17, -1.02]\t[-0.85, -1.04, -1.21, -0.93]\t[-0.45, -0.52, -0.84, -0.5]\t[-0.25, -0.17, -0.16, -0.94]\t[1.41, -0.22, 0.19, -0.25]\t\n",
      "[-1.03, -1.54, -1.47, -1.43]\t[-0.68, -0.67, -0.66, -0.84]\t[-0.67, 0.0, -0.66, -0.55]\t[-0.66, -0.99, -0.25, -1.23]\t[-0.95, -1.19, -0.41, -0.52]\t\n",
      "\n",
      "Episode 7 :\n",
      "[-0.3, 0.91, 0.44, -0.86]\t[0.19, -0.57, -0.3, -0.01]\t[1.62, 0, 0.07, 1.01]\t[1.19, 0.19, 2, 1]\t[0.94, 0, 0.25, 3]\t\n",
      "[-1.12, -0.63, -0.28, -0.07]\t[-0.74, -0.2, -1.51, -0.79]\t[-0.05, 0.66, -1.15, 0.63]\t[-0.03, 0.0, -0.07, 0.75]\t[-1.22, -0.64, 0.22, 0]\t\n",
      "[-0.87, -1.55, -1.32, -0.78]\t[-0.74, -1.1, -1.19, -1.19]\t[-0.75, -1.22, -0.61, -0.8]\t[0, 0, 0, 0]\t[-1.11, -0.76, -0.94, -1.04]\t\n",
      "[-1.73, -1.23, -1.17, -0.9]\t[-1.64, -1.41, -1.53, -0.93]\t[-0.94, -1.79, -1.36, -1.12]\t[-0.93, -1.34, -1.02, -0.94]\t[-0.59, -1.56, -0.78, -0.85]\t\n",
      "[-1.51, -1.54, -1.47, -1.43]\t[-1.67, -1.75, -1.72, -1.53]\t[-1.57, -1.78, -1.75, -1.47]\t[-1.48, -1.69, -1.8, -1.17]\t[-1.36, -1.19, -1.72, -1.04]\t\n",
      "\n",
      "Optimal state-action Q values :\n",
      "[-0.3, 0.91, 0.44, -0.86]\t[0.19, -0.57, -0.3, -0.01]\t[1.62, 0, 0.07, 1.01]\t[1.19, 0.19, 2, 1]\t[0.94, 0, 0.25, 3]\t\n",
      "[-1.12, -0.63, -0.28, -0.07]\t[-0.74, -0.2, -1.51, -0.79]\t[-0.05, 0.66, -1.15, 0.63]\t[-0.03, 0.0, -0.07, 0.75]\t[-1.22, -0.64, 0.22, 0]\t\n",
      "[-0.87, -1.55, -1.32, -0.78]\t[-0.74, -1.1, -1.19, -1.19]\t[-0.75, -1.22, -0.61, -0.8]\t[0, 0, 0, 0]\t[-1.11, -0.76, -0.94, -1.04]\t\n",
      "[-1.73, -1.23, -1.17, -0.9]\t[-1.64, -1.41, -1.53, -0.93]\t[-0.94, -1.79, -1.36, -1.12]\t[-0.93, -1.34, -1.02, -0.94]\t[-0.59, -1.56, -0.78, -0.85]\t\n",
      "[-1.51, -1.54, -1.47, -1.43]\t[-1.67, -1.75, -1.72, -1.53]\t[-1.57, -1.78, -1.75, -1.47]\t[-1.48, -1.69, -1.8, -1.17]\t[-1.36, -1.19, -1.72, -1.04]\t\n",
      "\n",
      "\n",
      "Q-learning\n",
      "=====\n",
      "\n",
      "Matrix representation of given grid world :\n",
      "0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t\n",
      "0\t0\t0\tx\t0\t\n",
      "0\ts\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t\n",
      "\n",
      "Arbitrarily initialized state-action Q values in the format ['→', '↓', '←', '↑'] :\n",
      "[0, 2, 3, 1]\t[4, 0, 3, 3]\t[2, 4, 4, 4]\t[3, 3, 2, 4]\t[1, 4, 0, 3]\t\n",
      "[0, 1, 1, 3]\t[1, 2, 4, 0]\t[2, 2, 4, 0]\t[3, 3, 0, 2]\t[2, 2, 4, 3]\t\n",
      "[1, 2, 4, 2]\t[2, 4, 3, 0]\t[3, 0, 1, 1]\t[0, 0, 0, 0]\t[3, 2, 2, 1]\t\n",
      "[0, 1, 2, 0]\t[2, 0, 4, 1]\t[0, 3, 0, 3]\t[4, 3, 4, 0]\t[2, 4, 1, 2]\t\n",
      "[2, 2, 1, 3]\t[1, 4, 0, 1]\t[0, 4, 2, 2]\t[4, 4, 0, 2]\t[3, 4, 2, 1]\t\n",
      "\n",
      "Intermediate state-action Q values :\n",
      "\n",
      "Episode 0 :\n",
      "[0, 2, 3, 1]\t[4, 0, 3, 3]\t[2, 4, 4, 4]\t[3, 3, 2, 4]\t[1, 4, 0, 3]\t\n",
      "[0, 1, 1, 3]\t[1, 2, 4, 0]\t[2, 2, 4, 0]\t[3, 3, 0, 2]\t[2, 2, 4, 3]\t\n",
      "[1, 2, 4, 2]\t[2, 4, 3, 0]\t[3, 0, 1, 1]\t[0, 0, 0, 0]\t[0.94, 0.22, -0.25, 1]\t\n",
      "[0, 1, 0.48, 0]\t[1.16, 1.12, 1.19, 1]\t[0, 1.88, 0, 3]\t[0.72, 3, 4, 0]\t[-0.02, 0.09, 1, 0.31]\t\n",
      "[0.75, 1.16, 1, 0.54]\t[1, 0.78, 0, 0.53]\t[1.12, 4, 2, 2]\t[-0.1, 0.78, 0, 1.53]\t[-0.13, 0.1, 0.06, -0.08]\t\n",
      "\n",
      "Episode 1 :\n",
      "[0, 2, 3, 1]\t[4, 0, 3, 3]\t[2, 4, 4, 4]\t[3, 3, 2, 4]\t[1, 4, 0, 3]\t\n",
      "[0, 1, 1, 3]\t[1, 2, 4, 0]\t[2, 2, 4, 0]\t[3, 3, 0, 2]\t[2, 2, 4, 3]\t\n",
      "[1, 2, 4, 2]\t[2, 4, 3, 0]\t[0.0, 0, 1, 1]\t[0, 0, 0, 0]\t[0.94, 0.22, -0.25, 1]\t\n",
      "[0, 0.04, 0.48, 0]\t[1.16, 1.12, 0.02, 1]\t[0, 1.88, 0, 1.41]\t[0.72, 3, 4, 0]\t[-0.02, 0.09, 1, 0.31]\t\n",
      "[-0.09, 0.08, -0.03, 0.54]\t[0.53, 0.09, 0, 0.53]\t[0.25, 0.78, 0.39, 1.16]\t[-0.73, 0.78, 0.19, 1.53]\t[-0.13, -0.68, -0.02, -0.08]\t\n",
      "\n",
      "Episode 2 :\n",
      "[0, 2, 3, 1]\t[4, 0, 3, 3]\t[2, 4, 4, 4]\t[3, 3, 2, 4]\t[1, 4, 0, 3]\t\n",
      "[0, 1, 1, 3]\t[1, 2, 4, 0]\t[2, 2, 4, 0]\t[3, 3, 0, 2]\t[2, 2, 4, 3]\t\n",
      "[1, -0.03, 4, 2]\t[2, 0.78, 1.88, 0]\t[0.0, 0, 1.38, 1]\t[0, 0, 0, 0]\t[0.94, 0.22, -0.25, 1]\t\n",
      "[-0.22, -0.7, -0.59, 0]\t[0.42, -1.19, 0.02, 0.91]\t[-0.75, -0.7, -0.33, 0.07]\t[-0.1, -1.12, -0.62, -0.75]\t[-0.02, 0.09, 1.38, 0.31]\t\n",
      "[-0.86, 0.08, -0.03, -0.83]\t[-0.81, -0.69, -0.5, -0.37]\t[-1.06, -0.89, -0.4, -0.58]\t[-0.94, -0.89, -0.95, -0.91]\t[-0.13, -0.68, -0.27, -0.08]\t\n",
      "\n",
      "Episode 3 :\n",
      "[0, 0.41, 0.47, -0.03]\t[2.12, 0, 3, 3]\t[2, 2.12, 4, 4]\t[3, 3, 2, 4]\t[1, 4, 0, 3]\t\n",
      "[0.47, 0.04, 1, -0.03]\t[1, 0.69, 0.78, 1.12]\t[1.16, 2, 4, 0]\t[1.88, 0.0, 0, 2]\t[2, 2, 1.66, 3]\t\n",
      "[1, -0.03, 0.25, 0.01]\t[2, 0.78, 1.59, -0.28]\t[0.0, 0, 0.53, 1]\t[0, 0, 0, 0]\t[0.94, 0.22, -0.25, 1]\t\n",
      "[-0.22, -0.7, -0.59, 0.19]\t[-0.61, -1.19, 0.02, 0.91]\t[-0.75, -0.7, -0.33, -0.09]\t[-0.1, -1.12, -0.62, -0.75]\t[-0.02, 0.09, 1.38, 0.31]\t\n",
      "[-0.86, 0.08, -0.03, -0.83]\t[-0.81, -0.69, -0.5, -0.37]\t[-1.06, -0.89, -0.4, -0.58]\t[-0.94, -0.89, -0.95, -0.91]\t[-0.13, -0.68, -0.27, -0.08]\t\n",
      "\n",
      "Episode 4 :\n",
      "[0, 0.41, -0.68, -0.29]\t[0.43, -0.31, 0.22, -0.56]\t[0.54, 0.14, -0.47, 0.78]\t[1.59, 0.54, -0.35, 0.43]\t[1, 1.03, 0.08, 0.78]\t\n",
      "[-0.11, -0.95, -1.45, -0.54]\t[-0.25, -0.94, -0.95, -0.31]\t[-0.1, -0.74, -0.09, 0.32]\t[0.47, 0.0, -0.39, 0.48]\t[0.22, -0.54, 1.66, -0.0]\t\n",
      "[-1.14, -1.03, -1.19, -0.95]\t[-0.88, -1.17, -1.09, -0.76]\t[-0.75, -0.79, -0.73, -0.72]\t[0, 0, 0, 0]\t[0.13, 0.22, -0.25, 0.12]\t\n",
      "[-1.21, -0.7, -0.86, -1.17]\t[-1.02, -1.28, -0.66, -0.82]\t[-0.75, -0.7, -1.14, -0.69]\t[-0.1, -1.12, -0.62, -0.75]\t[-0.02, -0.77, 1.38, -0.25]\t\n",
      "[-1.36, -1.37, -1.3, -1.23]\t[-0.81, -1.44, -0.99, -0.97]\t[-1.06, -0.89, -0.4, -0.58]\t[-0.94, -0.89, -0.95, -0.91]\t[-0.13, -0.68, -0.27, -0.12]\t\n",
      "\n",
      "Episode 5 :\n",
      "[0, 0.41, -0.68, -0.29]\t[0.43, -0.31, 0.22, -0.56]\t[0.54, 0.14, -0.47, 0.78]\t[0.13, 0.54, -0.35, 0.43]\t[1, 0.29, 0.08, 0.78]\t\n",
      "[-0.11, -0.95, -1.45, -0.54]\t[-0.25, -0.94, -0.95, -0.31]\t[-0.55, -0.74, -0.09, 0.32]\t[-0.53, 0.0, -0.39, 0.12]\t[-0.59, -0.54, -0.11, -0.0]\t\n",
      "[-1.14, -1.03, -1.19, -0.95]\t[-1.31, -1.17, -1.09, -0.76]\t[-0.75, -0.79, -0.73, -0.78]\t[0, 0, 0, 0]\t[0.13, 0.22, -0.81, 0.12]\t\n",
      "[-1.44, -1.5, -0.86, -1.17]\t[-1.02, -1.28, -1.24, -1.31]\t[-0.75, -0.7, -1.14, -0.69]\t[-0.1, -1.12, -0.62, -0.75]\t[-0.02, -0.77, 1.38, -0.25]\t\n",
      "[-1.36, -1.37, -1.3, -1.46]\t[-0.81, -1.44, -0.99, -0.97]\t[-1.06, -0.89, -0.4, -0.58]\t[-0.94, -0.89, -0.95, -0.91]\t[-0.13, -0.68, -0.27, -0.12]\t\n",
      "\n",
      "Episode 6 :\n",
      "[0, -0.7, -0.68, -0.29]\t[0.43, -0.31, -0.5, -0.56]\t[-0.36, 0.14, -0.47, -0.6]\t[0.13, -0.56, -0.35, -0.64]\t[1, 0.29, 0.08, 0.78]\t\n",
      "[-0.89, -0.95, -1.45, -0.54]\t[-1.06, -0.94, -0.95, -0.63]\t[-0.55, -0.74, -1.07, -0.57]\t[-0.53, -0.75, -0.39, -0.52]\t[-0.59, -0.54, -0.11, -0.0]\t\n",
      "[-1.14, -1.03, -1.19, -0.95]\t[-1.31, -1.17, -1.09, -1.06]\t[-0.75, -0.79, -1.29, -0.78]\t[0, 0, 0, 0]\t[0.13, 0.22, -0.81, 0.12]\t\n",
      "[-1.44, -1.5, -0.86, -1.17]\t[-1.33, -1.28, -1.24, -1.31]\t[-0.75, -0.7, -1.14, -1.26]\t[-0.1, -1.12, -0.62, -0.75]\t[-0.02, -0.77, 1.38, -0.25]\t\n",
      "[-1.36, -1.37, -1.3, -1.46]\t[-0.81, -1.44, -0.99, -0.97]\t[-1.06, -0.89, -0.4, -0.58]\t[-0.94, -0.89, -0.95, -0.91]\t[-0.13, -0.68, -0.27, -0.12]\t\n",
      "\n",
      "Episode 7 :\n",
      "[0, -0.7, -0.68, -0.29]\t[0.43, -0.31, -0.5, -0.56]\t[-0.36, 0.14, -0.47, -0.6]\t[0.13, -0.56, -0.35, -0.64]\t[1, 0.29, 0.08, 0.78]\t\n",
      "[-0.89, -0.95, -1.45, -0.54]\t[-1.06, -0.94, -0.95, -0.63]\t[-0.55, -0.74, -1.07, -0.57]\t[-0.53, -0.75, -0.39, -0.52]\t[-0.59, -0.54, -0.11, -0.0]\t\n",
      "[-1.14, -1.03, -1.72, -0.95]\t[-1.43, -1.17, -1.09, -1.06]\t[-0.94, -0.79, -1.57, -0.78]\t[0, 0, 0, 0]\t[0.13, 0.22, -0.81, 0.12]\t\n",
      "[-1.72, -1.5, -1.57, -1.49]\t[-1.33, -1.67, -1.46, -1.31]\t[-0.75, -0.7, -1.14, -1.42]\t[-0.1, -1.12, -0.62, -0.75]\t[-0.02, -0.77, 1.38, -0.25]\t\n",
      "[-1.47, -1.73, -1.86, -1.66]\t[-1.31, -1.58, -1.64, -1.59]\t[-1.06, -0.89, -1.3, -1.22]\t[-0.94, -0.89, -0.95, -0.91]\t[-0.13, -0.68, -0.27, -0.12]\t\n",
      "\n",
      "Optimal state-action Q values :\n",
      "[0, -0.7, -0.68, -0.29]\t[0.43, -0.31, -0.5, -0.56]\t[-0.36, 0.14, -0.47, -0.6]\t[0.13, -0.56, -0.35, -0.64]\t[1, 0.29, 0.08, 0.78]\t\n",
      "[-0.89, -0.95, -1.45, -0.54]\t[-1.06, -0.94, -0.95, -0.63]\t[-0.55, -0.74, -1.07, -0.57]\t[-0.53, -0.75, -0.39, -0.52]\t[-0.59, -0.54, -0.11, -0.0]\t\n",
      "[-1.14, -1.03, -1.72, -0.95]\t[-1.43, -1.17, -1.09, -1.06]\t[-0.94, -0.79, -1.57, -0.78]\t[0, 0, 0, 0]\t[0.13, 0.22, -0.81, 0.12]\t\n",
      "[-1.72, -1.5, -1.57, -1.49]\t[-1.33, -1.67, -1.46, -1.31]\t[-0.75, -0.7, -1.14, -1.42]\t[-0.1, -1.12, -0.62, -0.75]\t[-0.02, -0.77, 1.38, -0.25]\t\n",
      "[-1.47, -1.73, -1.86, -1.66]\t[-1.31, -1.58, -1.64, -1.59]\t[-1.06, -0.89, -1.3, -1.22]\t[-0.94, -0.89, -0.95, -0.91]\t[-0.13, -0.68, -0.27, -0.12]\t\n"
     ]
    }
   ],
   "source": [
    "print(\"SARSA\\n=====\\n\")\n",
    "stateActionMatrix = sarsa(5, (3,1), (2,3), -1, 0.625, 0.75, 0.3, 4, 8)\n",
    "print(\"\\nOptimal state-action Q values :\")\n",
    "showMatrix(stateActionMatrix)\n",
    "\n",
    "print(\"\\n\\nQ-learning\\n=====\\n\")\n",
    "stateActionMatrix = qLearning(5, (3,1), (2,3), -1, 0.625, 0.75, 0.3, 4, 8)\n",
    "print(\"\\nOptimal state-action Q values :\")\n",
    "showMatrix(stateActionMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
